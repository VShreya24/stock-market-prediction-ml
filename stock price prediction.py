# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uc08UV-ZZ9rPPdtQ9HxTw3IK4gPYwxOw
"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import numpy as np

# Load your dataset
df_stock = pd.read_csv('D:/Projects/Programming/infolimpioavanzadoTarget.csv')

# Separate input features and target variable
X = df_stock.drop(['TARGET', 'date', 'ticker'], axis=1)  # Assuming 'date' and 'ticker' are not useful features
y = df_stock['TARGET']

# Step 1: Remove rows with missing values
X_cleaned = X.dropna().copy()

# Ensure y matches the cleaned X data
y_cleaned = y[X_cleaned.index]

# Step 2: Replace infinite values with NaN, then drop them (using .loc to avoid warning)
X_cleaned = X_cleaned.replace([np.inf, -np.inf], np.nan)
X_cleaned = X_cleaned.dropna()

# Also update y_cleaned accordingly
y_cleaned = y_cleaned.loc[X_cleaned.index]

# Step 3: Handle Class Imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_cleaned, y_cleaned)

# Step 4: Feature Scaling using StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Step 5: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.3, random_state=42)

# Function to evaluate and print performance metrics
def evaluate_model(model, X_train, X_test, y_train, y_test):
    # Train the model
    model.fit(X_train, y_train)
    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate performance metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Print classification report
    print(f"\nModel: {model.__class__.__name__}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Step 6: Train and evaluate RandomForestClassifier
rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)
evaluate_model(rf_model, X_train, X_test, y_train, y_test)

# Step 7: Train and evaluate Support Vector Classifier (SVM)
svm_model = SVC(kernel='linear', class_weight='balanced', random_state=42)
evaluate_model(svm_model, X_train, X_test, y_train, y_test)

# Step 8: Train and evaluate Logistic Regression
log_model = LogisticRegression(class_weight='balanced', random_state=42)
evaluate_model(log_model, X_train, X_test, y_train, y_test)

# Step 9: Check feature importance for RandomForest to ensure no overfitting to specific features
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot feature importance
plt.figure(figsize=(10,6))
plt.title("Random Forest - Feature Importance")
plt.bar(range(X.shape[1]), importances[indices])
plt.show()